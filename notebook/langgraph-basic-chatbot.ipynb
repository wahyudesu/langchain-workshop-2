{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "pip install -U langgraph langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79ff15",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50321097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don’t skeletons fight each other?\n",
      "\n",
      "They don’t have the guts!\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"Tell me a joke\")\n",
    "# print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b922eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Python vs JavaScript  \n",
      "*(Sebuah penjelasan singkat, mudah dipahami, dan penuh semangat belajar!)*\n",
      "\n",
      "| Aspek | **Python** | **JavaScript** |\n",
      "|-------|------------|----------------|\n",
      "| **Asal‑muka** | Bahasa pemrograman tingkat tinggi, dibuat oleh Guido van Rossum (1991). | Bahasa skrip yang awalnya dikembangkan untuk browser (1995). |\n",
      "| **Paradigma** | Multi‑paradigma: prosedural, berorientasi objek, dan fungsional. | Multi‑paradigma: event‑driven, berorientasi objek (prototype), dan fungsional. |\n",
      "| **Sintaks** | Sangat bersih, menggunakan indentasi (spasi/tab) sebagai blok kode. | Menggunakan kurung kurawal `{}` dan titik koma opsional. |\n",
      "| **Eksekusi** | Interpreter (CPython, PyPy, dll.) → bytecode → mesin virtual. | Browser (Chrome, Firefox, Edge) atau Node.js → mesin JavaScript (V8, SpiderMonkey). |\n",
      "| **Penggunaan Utama** | Data science, machine learning, web backend (Django, Flask), scripting, automasi, game, dll. | Frontend web (DOM, interaksi UI), backend (Node.js), mobile (React Native), desktop (Electron). |\n",
      "| **Ekosistem** | `pip` + `PyPI` (lebih dari 300k paket). | `npm` + `yarn` (lebih dari 1 juta paket). |\n",
      "| **Typing** | Dinamis, tapi sekarang ada *type hints* (PEP 484) dan *mypy* untuk static typing. | Dinamis, tapi ada *TypeScript* (superset statically typed). |\n",
      "| **Kinerja** | Lebih lambat dibandingkan JavaScript di browser, tapi cukup cepat untuk banyak aplikasi. | Biasanya lebih cepat di browser karena JIT compilation, tapi di Node.js juga sangat cepat. |\n",
      "| **Learning Curve** | Mudah dipelajari, sintaks sederhana, banyak tutorial. | Lebih kompleks (closure, event loop, async/await), tapi sangat powerful. |\n",
      "| **Contoh Kode** | ```python<br>def greet(name):<br>    print(f\"Hello, {name}!\")<br>greet(\"Dewi\")``` | ```javascript<br>function greet(name) {<br>  console.log(`Hello, ${name}!`);<br>}<br>greet('Dewi');``` |\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Bagaimana Cara Kerjanya?**\n",
      "\n",
      "| Bahasa | Proses Eksekusi |\n",
      "|--------|-----------------|\n",
      "| **Python** | 1. **Source code** → 2. **Bytecode** (disimpan di `.pyc`) → 3. **Python Virtual Machine** (PVM) mengeksekusi bytecode. |\n",
      "| **JavaScript** | 1. **Source code** → 2. **Parsing** → 3. **JIT (Just‑In‑Time) compilation** → 4. **Eksekusi** di mesin (V8, SpiderMonkey). |\n",
      "\n",
      "> **Tip belajar**: Untuk Python, coba jalankan `python -m venv env` lalu `source env/bin/activate`.  \n",
      "> Untuk JavaScript, install Node.js lalu jalankan `node app.js`.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Kapan Harus Menggunakan Python?**\n",
      "\n",
      "| Situasi | Kenapa Python? |\n",
      "|---------|----------------|\n",
      "| **Data Science** | Library `pandas`, `numpy`, `scikit-learn`, `tensorflow`. |\n",
      "| **Web Backend** | Framework `Django`, `Flask`. |\n",
      "| **Automasi & Scripting** | `os`, `shutil`, `subprocess`. |\n",
      "| **Game Development** | `pygame`, `panda3d`. |\n",
      "| **Prototyping Cepat** | Sintaks sederhana, banyak contoh. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Kapan Harus Menggunakan JavaScript?**\n",
      "\n",
      "| Situasi | Kenapa JavaScript? |\n",
      "|---------|--------------------|\n",
      "| **Web Frontend** | Interaksi UI, manipulasi DOM, AJAX. |\n",
      "| **Web Backend** | `Node.js` + `Express`, `NestJS`. |\n",
      "| **Mobile Apps** | `React Native`, `Ionic`. |\n",
      "| **Desktop Apps** | `Electron`. |\n",
      "| **Realtime Apps** | WebSocket, Socket.io. |\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Perbandingan Praktis: Contoh Kode**\n",
      "\n",
      "| Tugas | Python | JavaScript |\n",
      "|-------|--------|------------|\n",
      "| **Loop 1‑10** | ```python<br>for i in range(1, 11):<br>    print(i)``` | ```javascript<br>for (let i = 1; i <= 10; i++) {<br>  console.log(i);<br>}``` |\n",
      "| **Async API Call** | ```python<br>import aiohttp, asyncio<br>async def fetch(url):<br>    async with aiohttp.ClientSession() as session:<br>        async with session.get(url) as resp:<br>            return await resp.text()<br>asyncio.run(fetch('https://api.example.com'))``` | ```javascript<br>async function fetchData(url) {<br>  const response = await fetch(url);<br>  const data = await response.json();<br>  return data;<br>}<br>fetchData('https://api.example.com');``` |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Kelebihan & Kekurangan**\n",
      "\n",
      "| Bahasa | Kelebihan | Kekurangan |\n",
      "|--------|-----------|------------|\n",
      "| **Python** | - Sintaks bersih<br>- Komunitas besar<br>- Library untuk hampir semua bidang | - Lebih lambat (CPU bound)<br>- Tidak se‑fleksibel JavaScript di browser |\n",
      "| **JavaScript** | - Eksekusi cepat di browser<br>- Bisa di backend (Node.js)<br>- Ekosistem npm sangat luas | - Sintaks bisa membingungkan (closure, hoisting)<br>- Debugging di browser kadang sulit |\n",
      "\n",
      "---\n",
      "\n",
      "### 6. **Tips Memilih Bahasa**\n",
      "\n",
      "1. **Tentukan Tujuan**  \n",
      "   - *Data science, AI, scripting* → Python.  \n",
      "   - *Web interaktif, mobile, desktop* → JavaScript.\n",
      "\n",
      "2. **Pertimbangkan Tim & Tooling**  \n",
      "   - Jika tim sudah familiar dengan satu bahasa, manfaatkan keahlian tersebut.\n",
      "\n",
      "3. **Pelajari Dasar‑dasarnya**  \n",
      "   - Python: `def`, `class`, `list`, `dict`.  \n",
      "   - JavaScript: `function`, `class`, `Array`, `Object`.\n",
      "\n",
      "4. **Gunakan Type System**  \n",
      "   - Python: `typing` + `mypy`.  \n",
      "   - JavaScript: `TypeScript`.\n",
      "\n",
      "---\n",
      "\n",
      "## Kesimpulan\n",
      "\n",
      "- **Python**: Bahasa yang *membuatmu cepat menulis kode* dan *mudah dipahami*, cocok untuk *data science*, *backend*, dan *automasi*.  \n",
      "- **JavaScript**: Bahasa *paling populer di web*, memungkinkan *interaksi real‑time* dan *cross‑platform* (web, mobile, desktop).\n",
      "\n",
      "Keduanya memiliki kekuatan masing‑masing. Pilihlah berdasarkan **apa yang ingin kamu capai** dan **di mana kamu akan menjalankan kode**. Dan ingat, tidak ada yang salah dengan belajar keduanya—keduanya akan memperkaya skill programmingmu!\n",
      "\n",
      "---\n",
      "\n",
      "> **Saran belajar**:  \n",
      "> 1. Buat proyek kecil (misalnya, *todo list*) menggunakan Python (Flask) dan JavaScript (React).  \n",
      "> 2. Ikuti tutorial online (freeCodeCamp, Coursera, Udemy).  \n",
      "> 3. Bergabunglah dengan komunitas (Discord, Reddit, Stack Overflow).  \n",
      "\n",
      "Selamat belajar, dan semoga sukses! 🚀\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "system_msg = SystemMessage(\n",
    "    \"kamu adalah guru yang baik hati dan pintar, kamu membantu siswamu dalam belajar soal pemrograman.\"\n",
    ")\n",
    "human_msg = HumanMessage(\"apa bedanya python sama javascript?\")\n",
    "\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "# print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85d041",
   "metadata": {},
   "source": [
    "`HumanMessage`\n",
    "A message sent from the perspective of the human, with the user role\n",
    "\n",
    "`AIMessage`\n",
    "A message sent from the perspective of the AI that the human is interacting with, with the assistant role\n",
    "\n",
    "`SystemMessage`\n",
    "A message setting the instructions the AI should follow, with the system role\n",
    "\n",
    "`ChatMessage`\n",
    "A message allowing for arbitrary setting of role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfe2205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the question based on the\\n    context below. If the question cannot be answered using the information \\n    provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large \\n        Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n        models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the\n",
    "    context below. If the question cannot be answered using the information \n",
    "    provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''Answer the question based on the context below. If the \n",
    "        question cannot be answered using the information provided, answer with \n",
    "        \"I don\\'t know\".'''),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fd6a3",
   "metadata": {},
   "source": [
    "Prompt community\n",
    "\n",
    "https://smith.langchain.com/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10096fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = Client(api_key=os.environ.get(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"hardkothari/prompt-maker\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cf8dc",
   "metadata": {},
   "source": [
    "## Specific Formats out of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc98b5",
   "metadata": {},
   "source": [
    "kita pengen si AI tuh :\n",
    "- cuman jawab ya dan tidak\n",
    "- milih dari jenis jenis yang kita tentukan\n",
    "- generate json data\n",
    "- generate code python/javascript\n",
    "- only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae12f51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='They weigh the same – both are one pound.', justification='A pound is a unit of mass, so a pound of bricks and a pound of feathers each have the same mass of one pound. The difference is in volume and density, not weight.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user's question along with justification for the \n",
    "        answer.'''\n",
    "    answer: str\n",
    "    '''The answer to the user's question'''\n",
    "    justification: str\n",
    "    '''Justification for the answer'''\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound \n",
    "    of feathers\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa42217",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "There is a common interface with these methods:\n",
    "\n",
    "`invoke`: transforms a single input into an output\n",
    "\n",
    "`batch`: efficiently transforms multiple inputs into multiple outputs\n",
    "\n",
    "`stream`: streams output from a single input as it’s produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b02d363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! 👋 How can I help you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "completion = model.invoke('Hi there!') \n",
    "# Hi!\n",
    "\n",
    "# completions = model.batch(['Hi there!', 'Bye!'])\n",
    "# # ['Hi!', 'See you!']\n",
    "\n",
    "# for token in model.stream('Bye!'):\n",
    "#     print(token.content)\n",
    "#     # Good\n",
    "#     # bye\n",
    "#     # !\n",
    "\n",
    "completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e47f0274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! 👋 How can I help you today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions = model.batch(['Hi there!', 'Bye!'])\n",
    "\n",
    "completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1763fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why\n",
      " don\n",
      "’t\n",
      " skeleton\n",
      "s\n",
      " fight\n",
      " each\n",
      " other\n",
      "?\n",
      "\n",
      "\n",
      "They\n",
      " don\n",
      "’t\n",
      " have\n",
      " the\n",
      " guts\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream('Tell me a joke!'):\n",
    "    # IMPORTANT: Keep the processing of each chunk as efficient as possible.\n",
    "    # While you're processing the current chunk, the upstream component is\n",
    "    # waiting to produce the next one. For example, if working with LangGraph,\n",
    "    # graph execution is paused while the current chunk is being processed.\n",
    "    # In extreme cases, this could even result in timeouts (e.g., when llm outputs are\n",
    "    # streamed from an API that has a timeout).\n",
    "    print(token.content)\n",
    "    # Good\n",
    "    # bye\n",
    "    # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691102e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the parrot bring did the parrot bring a ladder to the bar?\n",
      "\n",
      " a ladder to the bar?\n",
      "\n",
      "Because it heard the drinksBecause it heard the drinks were on the house! 🦜 were on the house! 🦜🍹🍹"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for event in chain.astream_events({\"topic\": \"parrot\"}):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        # Ambil content dari chunk\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400dac5",
   "metadata": {},
   "source": [
    "RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"document.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed031dcf",
   "metadata": {},
   "source": [
    "Text splitters\n",
    "Text Splitters take a document and split into chunks that can be used for retrieval.\n",
    "\n",
    "How to: recursively split text\n",
    "How to: split HTML\n",
    "How to: split by character\n",
    "How to: split code\n",
    "How to: split Markdown by headers\n",
    "How to: recursively split JSON\n",
    "How to: split text into semantic chunks\n",
    "How to: split by tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "splitted_docs = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876adc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "model = OpenAIEmbeddings()\n",
    "\n",
    "embeddings = model.embed_documents([\n",
    "    \"Hi there!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"What's your name?\",\n",
    "    \"My friends call me World\",\n",
    "    \"Hello World!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae2f5d",
   "metadata": {},
   "source": [
    "[\n",
    "  [\n",
    "    -0.004845875, 0.004899438, -0.016358767, -0.024475135, -0.017341806,\n",
    "      0.012571548, -0.019156644, 0.009036391, -0.010227379, -0.026945334,\n",
    "      0.022861943, 0.010321903, -0.023479493, -0.0066544134, 0.007977734,\n",
    "    0.0026371893, 0.025206111, -0.012048521, 0.012943339, 0.013094575,\n",
    "    -0.010580265, -0.003509951, 0.004070787, 0.008639394, -0.020631202,\n",
    "    ... 1511 more items\n",
    "  ]\n",
    "  [\n",
    "      -0.009446913, -0.013253193, 0.013174579, 0.0057552797, -0.038993083,\n",
    "      0.0077763423, -0.0260478, -0.0114384955, -0.0022683728, -0.016509168,\n",
    "      0.041797023, 0.01787183, 0.00552271, -0.0049789557, 0.018146982,\n",
    "      -0.01542166, 0.033752076, 0.006112323, 0.023872782, -0.016535373,\n",
    "      -0.006623321, 0.016116094, -0.0061090477, -0.0044155475, -0.016627092,\n",
    "    ... 1511 more items\n",
    "  ]\n",
    "  ... 3 more items\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "## Load the document \n",
    "\n",
    "loader = TextLoader(\"./test.txt\")\n",
    "doc = loader.load()\n",
    "\n",
    "\"\"\"\n",
    "[\n",
    "    Document(page_content='Document loaders\\n\\nUse document loaders to load data \n",
    "        from a source as `Document`\\'s. A `Document` is a piece of text\\nand \n",
    "        associated metadata. For example, there are document loaders for \n",
    "        loading a simple `.txt` file, for loading the text\\ncontents of any web \n",
    "        page, or even for loading a transcript of a YouTube video.\\n\\nEvery \n",
    "        document loader exposes two methods:\\n1. \"Load\": load documents from \n",
    "        the configured source\\n2. \"Load and split\": load documents from the \n",
    "        configured source and split them using the passed in text \n",
    "        splitter\\n\\nThey optionally implement:\\n\\n3. \"Lazy load\": load \n",
    "        documents into memory lazily\\n', metadata={'source': 'test.txt'})\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "## Split the document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "## Generate embeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [chunk.page_content for chunk in chunks]\n",
    ")\n",
    "\"\"\"\n",
    "[[0.0053587136790156364,\n",
    " -0.0004999046213924885,\n",
    "  0.038883671164512634,\n",
    " -0.003001077566295862,\n",
    " -0.00900818221271038, ...], ...]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ac2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus database postgres\n",
    "# first, pip install langchain-postgres\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "db = PGVector.from_documents(chunks, embeddings_model, connection=connection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
